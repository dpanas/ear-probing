import logging
import pickle

import time as tm

from itertools import product

from .chat_api import get_model_answer, get_chat_answer
from .prompt_parsing import fill_query_template, extract_convert

logger = logging.getLogger(__name__)

def get_valid_pairs( items):
    return set([tuple(sorted(x)) for x in list(product( items, items)) if x[0] != x[1]])


class NumericalEntailmentBase():
    
    @staticmethod
    def get_valid_subjects_parts_probes( subjects_dict):
        subjects_pairs = get_valid_pairs( subjects_dict.keys())
        subjects_pairs_part_pairs = {}
        probes = []
        for subject_a, subject_b in subjects_pairs:
            all_parts = set( subjects_dict[ subject_a].keys()) | set( subjects_dict[ subject_b].keys())
            part_pairs_ = list( product( all_parts, all_parts))
            subjects_pairs_part_pairs[ (subject_a, subject_b)] = part_pairs_
            probes.extend( 
                [(subject_a, subject_b, x, y) for x, y in part_pairs_]
            )
        return subjects_pairs_part_pairs, probes
    
    @staticmethod
    def get_all_parts( subjects_dict):
        all_parts = []
        _ = [all_parts.extend( di_.keys()) for di_ in subjects_dict.values()]
        return sorted(set(all_parts))
    
    @staticmethod
    def entailment( num_a, num_b):
        if num_a > num_b:
            return 'more'
        elif num_a < num_b:
            return 'less'
        else:
            return 'same'
    
    def __init__( self, subjects, model, fact_1= None, prompt_1= None, fact_2= None, prompt_2= None, test_mode= False):
        """
        Initialise the 'Probator' with a dictionary of subjects-elements. A list of probes will be
        generated by getting a product of subject pairs (non-repeating) and then for each
        element pairs (repeating), for asking questions like 'a typical human has a number of
        fingers that is <> than the number of wheels that a car has'.
        
        :param subjects: dictionary of subjects, e.g. bike, cat, values
        being a defaultdict listing all non-zero elements / parts and their numbers
        (otherwise defaulting to 0)
        :param moedel: str name of model to probe
        """
        
        self.model = model
        self.subjects = subjects
        self.all_parts = self.get_all_parts( self.subjects)
        self.subjects_pairs_part_pairs, self.probes = self.get_valid_subjects_parts_probes( self.subjects)
        logger.debug(f'Initialised {self.__class__.__name__} with {len( self.probes)} probes')
        
        self.test_mode= test_mode
        
        # how will it be prompting for answers (if None, will use default settings of other code..):
        self.fact_form_1 = fact_1
        self.fact_form_2 = fact_2
        self.prompt_1 = prompt_1
        self.prompt_2 = prompt_2
        mask_flag = ['<mask>' in x for x in [fact_1, prompt_1, fact_2, prompt_2]]
        if all( mask_flag):
            self.query_type = 'mask'
        elif not any( mask_flag):
            self.query_type = 'qa'
        
        # for answers:
        self.probes_answers = {}
        self.subjects_answers = {}
    
    def parse_numbers( self, mode= 'lenient'):
        preamble_1 = self.prompt_1.split('{sentence}')[0].split('\n')[-1]
        for sub in self.subjects_answers.keys():
            for k, v in self.subjects_answers[ sub].items():
                orig = v[-1]
                q = fill_query_template( 
                    self.fact_form_1, **dict(zip(['thing','elements'],[sub,k]))
                )
                num = extract_convert( q, orig, self.query_type,'num', preamble_1, k, mode)
                self.subjects_answers[ sub][k] = [num, orig]
                
        preamble_2 = self.prompt_2.split('{sentence}')[0].split('\n')[-1]
        for probe, answers in self.probes_answers.items():
            new_answers = []
            orig = answers[-1]
            for ii in [0,1]:
                sub, ele = probe[0+ii], probe[2+ii]
                new_answers.append( self.subjects_answers[ sub][ele][0])
            q = fill_query_template(
                self.fact_form_2, **dict(zip(['thing_a','thing_b','elements_a','elements_b'], probe))
            )
            if orig is not None:
                rel_ = extract_convert( q, orig, self.query_type, 'rel', preamble_2, mode)
            else:
                # left-over from earlier attempts.
                rel_ = extract_convert( q, answers[-2], self.query_type, 'rel', preamble_2, self.model)
            self.probes_answers[ probe] = new_answers + [rel_,orig]
    
    def batch_probes_truth( self):
        """
        Evaluate the ground truth entailment (a.k.a. actual relationship between actual numbers).
        """
        self.probes_truth = {}
        for probe in self.probes:
            subject_a, subject_b, element_a, element_b = probe
            num_a, num_b = self.subjects[ subject_a][ element_a], self.subjects[ subject_b][ element_b]
            self.probes_truth[ probe] = [ num_a, num_b, self.entailment( num_a, num_b)]  
    
    @classmethod
    def load( self, path):
        with open( path, 'rb') as iin:
            return pickle.load( iin)
    
    def save( self, path):
        with open( path, 'wb') as oot:
            pickle.dump( self, oot)  

            
class NumericalEntailmentBatchProbe( NumericalEntailmentBase):  
    
    def __init__( self, subjects, model, fact_1= None, prompt_1= None, fact_2= None, prompt_2= None, test_mode= False):
        super().__init__(
            subjects, model, fact_1, prompt_1, fact_2, prompt_2, test_mode
        )
        self.batch_probes_truth()
        logger.debug(f'Added ground truth entailment to {self.__class__.__name__}.')
        self.subjects_answers = { k:{} for k in self.subjects.keys()}     
        
        
    def batch_numbers( self, pipeline):
        probes_to_eval = []
        for t, e in product( self.subjects.keys(), self.all_parts):
            try:
                self.subjects_answers[t]
                self.subjects_answers[t][e]
            except KeyError:
                probes_to_eval.append([t,e])
        
        
        start = tm.time()   
        sents = [
            (t, e, fill_query_template( self.fact_form_1, **dict(zip(['thing','elements'], [t,e]))))
            for t,e in probes_to_eval
        ]
        prompts = [
            self.prompt_1.format( sentence= tup_[2]) for tup_ in sents
        ]
        if self.test_mode:
            answers = [x.replace('<mask>','several') if '<mask>' in x else 'six' for x in prompts]
        else:
            answers = get_model_answer( [x for x in prompts], pipeline)
        for ii, tup_ in enumerate( sents):
            subject, part, _ = tup_            
            self.subjects_answers[ subject][ part] = answers[ ii]
            
        logger.info(f'Took {(tm.time() - start)/60} minutes to run through {len( probes_to_eval)} numerical queries.')
    
    def batch_probes( self, pipeline):
        """
        Probe the model in batch mode (e.g. Mistral).
        """
        probes_to_eval = list( set( self.probes) - set( self.probes_answers.keys()))
        sents = [ 
            fill_query_template( self.fact_form_2, **dict( zip( ['thing_a','thing_b','elements_a','elements_b'], probe))) 
            for probe in probes_to_eval
        ]
        prompts = [
            self.prompt_2.format( sentence= sent) for sent in sents
        ]
        if self.test_mode:
            answers = [x.replace('<mask>','unsure#') if '<mask>' in x else 'more or less' for x in prompts]
        else:
            answers = get_model_answer( prompts, pipeline)
        for ii, probe in enumerate( probes_to_eval):
            self.probes_answers[ probe] = answers[ ii]  
            
        logger.info(f'Took {(tm.time() - start)/60} minutes to run through {len( probes_to_eval)} numerical queries.')      
    
    def __call__( self, file_path, pipeline):
        """
        Runs all probes (generated at init time from the subject data) and construct three
        dictionaries: a ground truth one (deterministically inferred from the data); answers
        of the model probed through the API; any exceptions. 
        NOTE: the method aborts if too many exceptions encountered, as it may be a bug, or 
        API hitting some limits. 
        """
        logger.debug(f'Probing model: {self.model}.')
        self.batch_numbers( pipeline)
        self.save( file_path)
        
        self.batch_probes( pipeline)
        self.save( file_path)
        

class NumericalEntailmentProbator( NumericalEntailmentBase):    
    
    def __init__( self, subjects, model, fact_1= None, prompt_1= None, fact_2= None, prompt_2= None, test_mode= False):
        super().__init__(
            subjects, model, fact_1, prompt_1, fact_2, prompt_2, test_mode
        )
        self.api_calls = 0
        self.exception_count = 0
        self.probes_truth = {}
        self.probes_exceptions = {}    
    
    def fetch_number( self, subject, elements):
        """
        Method for getting atomic numerical facts, either from the dictionary kept on
        the instance or from prompting. (Since generated data has multiple comparisons
        of same objects / atomic facts and using reproducible prompting setting, no 
        point asking chat each time)
        """
        try:
            self.subjects_answers[ subject][ elements]
        except KeyError as eek:
            missing_key = eek.args[0]
            try:
                query = fill_query_template( 
                    self.fact_form_1, **dict(zip(['thing','elements'], [ subject, elements]))
                )
                prompt = self.prompt_1.format( sentence= query)
                num = get_chat_answer( prompt, model= self.model)
                self.api_calls += 1
                if missing_key == subject:
                    self.subjects_answers[ subject] = { elements: [num]}
                elif missing_key == elements:
                    self.subjects_answers[ subject][ elements] = [num]
                return num
            except Exception as eek:
                self.probes_exceptions[ (subject,elements)] = eek
                self.exception_count += 1
    
    def fetch_probe( self, probe):
        """
        Run a numerical entailment probe and store answers, separately ground truth that is
        implied by the data (deterministically) and separately chat answer. A numerical entailment 
        probe is a tuple of 2 subjects and 2 elements, for comparing numbers of elements.
        """
        subject_a, subject_b, elements_a, elements_b = probe
        self.fetch_number( subject_a, elements_a), 
        self.fetch_number( subject_b, elements_b)
        try:
            query = fill_query_template( 
                self.fact_form_2, **dict( zip( ['thing_a','thing_b','elements_a','elements_b'], probe))
            )
            prompt = self.prompt_2.format( sentence= query)
            rel = get_chat_answer( prompt, model= self.model) 
            self.api_calls += 1
            self.probes_answers[ probe] = [rel]
        except Exception as eek:
            self.probes_exceptions[ probe] = eek
            self.exception_count +=1

    def __call__( self, file_path):
        """
        Runs all probes (generated at init time from the subject data) and construct three
        dictionaries: a ground truth one (deterministically inferred from the data); answers
        of the model probed through the API; any exceptions. 
        NOTE: the method aborts if too many exceptions encountered, as it may be a bug, or 
        API hitting some limits. 
        """
        logger.debug(f'Probing model: {self.model}.')
        start = tm.time()
        num_for_run = len( self.probes)
        abort = False
        num_ = 0
        logger.info(f'Have { num_for_run} probes to run.')
        while (len(self.probes) > 0) and not abort:
            probe = self.probes.pop(0)
            try:
                self.fetch_probe( probe)
                num_ += 1
            except Exception as eek:
                logger.warning(f'Un-handlable exception at {probe}, saving progress and re-raising!')
                self.save( file_path)
                raise eek
            if self.exception_count > max( 10, 0.1 * self.api_calls):
                logger.warning(f'After {self.api_calls} hit {self.exception_count}, aborting!')
                abort = True
            if num_ % 500 == 0:
                self.save( file_path)
                logger.info(f'Saved progress, {len(self.probes)} left to go...')
        took = (tm.time() - start) / 60
        logger.info(f'Took {took} mins to run through {num_for_run - len( self.probes)}.')
        
        if len( self.probes) == 0 and len( self.subjects_answers) == 0:
            logger.info(f'Probes have been run but not subjects!')
            for sub in self.subjects.keys():
                for elem in self.subjects[ sub].keys():
                    self.fetch_number( sub, elem)
            took = (tm.time() - start) / 60
            logger.info(f'Took {took} to run subjects')
        
        self.save( file_path)
        logger.info(f'Save yourselves! ==> { file_path}')
